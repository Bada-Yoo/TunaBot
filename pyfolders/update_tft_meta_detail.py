from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import os
import json

def crawl_detail_info():
    base_dir = os.path.dirname(__file__)
    meta_path = os.path.join(base_dir, "tft_meta.json")
    detail_path = os.path.join(base_dir, "tft_metadetail.json")

    with open(meta_path, "r", encoding="utf-8") as f:
        meta_data = json.load(f)

    detail_data = {"updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "meta": []}

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,3000")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    for entry in meta_data["meta"]:
        url = entry["url"]
        name = entry["name"]
        print(f"ğŸ” {name} ì„¸ë¶€ ì •ë³´ í¬ë¡¤ë§ ì¤‘...")

        driver.get(url)
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "challenger-comment"))
            )
        except:
            print(f"âŒ íƒ€ì„ì•„ì›ƒ: {url}")
            continue

        # ë‚œì´ë„
        try:
            difficulty_elem = driver.find_element(By.XPATH, "//h1[contains(text(), 'ë± ë‚œì´ë„')]/following-sibling::p")
            difficulty = difficulty_elem.text.strip()
        except:
            difficulty = ""

        # ë ˆë²¨ë§ ì¶”ì²œ
        leveling = []
        try:
            p_tags = driver.find_elements(By.XPATH, "//h1[contains(text(), 'ìŠ¤í…Œì´ì§€ë³„ ë ˆë²¨ì—… ì¶”ì²œ')]/following-sibling::p")
            for p in p_tags:
                text = p.get_attribute("innerText").strip()
                if text:
                    lines = text.split("\n")
                    for line in lines:
                        # "(ì—°ìŠ¹ or ì—°íŒ¨)" ì œê±°, ë‹¤ë¥¸ ê´„í˜¸ëŠ” ìœ ì§€
                        if "(ì—°ìŠ¹ or ì—°íŒ¨)" in line:
                            line = line.replace("(ì—°ìŠ¹ or ì—°íŒ¨)", "").strip()
                        if line:
                            leveling.append(line)
        except Exception as e:
            print(f"âš ï¸ ë ˆë²¨ë§ ì •ë³´ ì—†ìŒ: {e}")

        # ë± êµ¬ì„±
        try:
            composition_elem = driver.find_element(By.XPATH, "//h1[contains(text(), 'ë± êµ¬ì„±')]/following-sibling::p")
            composition = composition_elem.text.strip()
        except:
            composition = ""

        detail_data["meta"].append({
            "index": entry["index"],
            "name": name,
            "difficulty": difficulty,
            "leveling": leveling,
            "composition": composition
        })

    driver.quit()

    with open(detail_path, "w", encoding="utf-8") as f:
        json.dump(detail_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… metadetail.json ì €ì¥ ì™„ë£Œ ({detail_data['updated_at']} ê¸°ì¤€, {len(detail_data['meta'])}ê°œ ì¡°í•©)")

if __name__ == "__main__":
    crawl_detail_info()
